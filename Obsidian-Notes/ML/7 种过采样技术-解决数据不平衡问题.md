在模型训练时，数据不平衡是我们面临的主要挑战。数据的不平衡(即数据集中存在少数类)，使得模型会尝试学习多数类，并导致偏颇预测。

不平衡问题有一些著名例子：

-   信用卡欺诈检测
-   疾病诊断
-   垃圾邮件检测

针对数据不平衡的问题，有多种技术手段可以处理，主要有过采样、下采样或两者的组合。在本文中我将介绍 7 种采样技术并深入说明。**喜欢本文记得收藏、点赞、关注。**

**【注】文末提供技术交流群**

-   随机过采样（Random Over Sampling）
-   Smote
-   BorderLine Smote
-   KMeans Smote
-   SVM Smote
-   ADASYN
-   Smote-NC

### **1、随机过采样**

随机过采样是平衡数据集不平衡问题最简单过采样技术。它通过复制少数类示例来平衡数据。这不会导致任何信息丢失，但数据集在复制相同信息时容易过度拟合。

![](https://pic2.zhimg.com/80/v2-08f9d4d1d638eaab8ba370d0d7aaac45_720w.jpg)

左：随机过采样后散点图，右：随机过采样后模型的性能

### **2、SMOTE**

随机过采样很容易过度拟合，因为少数类样本被复制。而 SMOTE 是合成少数类的过采样技术，它创建新的合成样本以平衡数据集。

SMOTE 的工作原理是利用k最近邻域算法创建合成数据。使用 Smote 创建步骤示例：

-   确定要素矢量及其最近邻域
-   计算两个采样点之间的距离
-   将距离与随机数乘以 0 和 1 之间。
-   在计算距离上识别线段上的新点。
-   对已识别特征矢量重复此过程。

![](https://pic1.zhimg.com/80/v2-28f0acc2d2380e3b07af57d5a44e7cf4_720w.jpg)

左：SMOTE后散点图，右：SMOTE后模型性能

### **3、Borderline Smote**

由于多数等级点的区域内存在一些少数等级点或离群值，因此建立了少数等级点的桥梁。 在"Smote"的情况下这是一个问题，可以使用"Borderline Smote"解决。

在 Borderline Smote 技术中，仅对边界线附近的少数示例进行过度采样。它将少数类点分类为噪声点、边框点。噪声点是少数类点，其邻域的多数点占多数，边界点在其邻域具有多数和少数类点。Borderline Smote 算法仅尝试使用这些边界点创建合成点，并忽略噪声点。

![](https://pic3.zhimg.com/80/v2-253177d8f7a37d105276e32d843458d6_720w.jpg)

左：边界 SMOTE 之后散点图，右：边界 SMOTE 之后的模型性能

### **4、Kmeans Smote：**

KMeans SMOTE 是用于类不平衡数据的过采样方法。它通过在输入空间安全和关键区域生成少数类样本来辅助分类。该方法避免了噪声的产生，并有效地克服了类之间和类内部的不平衡。

KMeans SMOTE的工作分为五个步骤：

-   使用k均值聚类算法对整个数据进行聚类。
-   选择少数族裔样本数量很多的集群
-   将更多的合成样本分配给少数类样本稀少分布的群集。

在此，每个过滤后的群集都使用SMOTE进行过采样。

![](https://pic1.zhimg.com/80/v2-72d0ee28d31cdc9d119310cda254e160_720w.jpg)

左：KMeans SMOTE 之后散点图，右：KMeans SMOTE 之后的模型性能

### **5、SVM Smote**

Borderline SMOTE的另一个变体是 SVM SMOTE。该技术结合了SVM算法来识别错误分类点。

在SVM SMOTE中，在原始训练集上对SVM分类器进行训练后，边界区域由支持向量近似。然后，沿着将每个少数群体支持向量与多个其最近邻居连接的直线随机创建合成数据。

![](https://pic1.zhimg.com/80/v2-6a8a36264174aee4c6fb50a2ba8e7da0_720w.jpg)

左：SVM SMOTE后散点图，右：SVM SMOTE后模型的性能

### **6、ADASYN**

Borderline Smote更加重视或仅使用作为边界点的极端观测值来创建综合点，而忽略其余少数族点。ADASYN算法解决了这个问题，因为它根据数据密度创建合成数据。

综合数据生成与少数族裔的密度成反比。在少数族裔类别的低密度区域中，与高密度区域相比，生成的合成数据数量相对较多。

换句话说，在少数族密度较低的区域中，将创建更多的综合数据。

![](https://pic3.zhimg.com/80/v2-fd83a287368b6e192c068923fc9efc7e_720w.jpg)

左：ADASYN后散点图，右：ADASYN后模型的性能

### **7、Smote-NC**

过采样技术仅适用于具有所有连续特征的数据集。对于具有分类特征的数据集，我们有Smote的变体，即Smote-NC(标称和连续）。

通过单次热编码，Smote也可以用于具有分类特征的数据，但是它可能会导致维数增加。标签编码也可以用于将分类转换为数字，但是在冒烟之后，它可能会导致不必要的信息。这就是为什么当我们遇到混合数据的情况时，需要使用SMOTE-NC的原因。可以通过表示分类特征来使用Smote NC，并且Smote将重新采样分类数据，而不创建合成数据。

![](https://pic1.zhimg.com/80/v2-fb192c2c13ebc4c36764c4f0e30799f8_720w.jpg)

### **结论**

对不平衡数据集进行建模是我们在训练模型时面临的主要挑战，使用上面讨论的各种过采样技术可以提高模型的性能。 同样在本文中，我们讨论了SMOTE-NC，它是SMOTE的一种变体，可以处理分类特征。

还可以通过使用各种欠采样技术（例如随机欠采样，TomekLinks等）以及过采样和欠采样技术（例如SMOTEENN，SMOTETomek等）的组合来改善不平衡数据集的模型性能。本文使用的数据源为，需要可以获取！

```text
https://www.kaggle.com/shubh0799/churn-modelling
```