过采样：针对少样本类生成新的数据样本参与训练。欠采样：针对多样本的类筛选一部分样本参与训练。

---

**过采样(Over-sampling)**

**朴素随机过采样**

针对不平衡数据，最简单的一种方法就是生成少数类的样本，这其中最基本的一种方法就是: 从少数类的样本中进行随机采样来增加新的样本，RandomOverSampler函数就能实现上述的功能.

```text
from sklearn.datasets import make_classification
from collections import Counter
X, y = make_classification(n_samples=5000, n_features=2, n_informative=2,
                           n_redundant=0, n_repeated=0, n_classes=3,
                           n_clusters_per_class=1,
                           weights=[0.01, 0.05, 0.94],
                           class_sep=0.8, random_state=0)
Counter(y)
Out[10]: Counter({0: 64, 1: 262, 2: 4674})

from imblearn.over_sampling import RandomOverSampler

ros = RandomOverSampler(random_state=0)
X_resampled, y_resampled = ros.fit_sample(X, y)


sorted(Counter(y_resampled).items())
Out[13]:
[(0, 4674), (1, 4674), (2, 4674)]
```

以上就是通过简单的随机采样少数类的样本, 使得每类样本的比例为1:1:1。

针对分类过程中样本数量比较少的一类进行扩容：随机过采样到SMOTE与ADASYN。

SMOTE: 对于少数类样本a, 随机选择一个最近邻的样本b, 然后从a与b的连线上随机选取一个点c作为新的少数类样本；

ADASYN: 关注的是在那些基于K最近邻分类器被错误分类的原始样本附近生成新的少数类样本；

```text
from imblearn.over_sampling import SMOTE, ADASYN

X_resampled_smote, y_resampled_smote = SMOTE().fit_sample(X, y)

sorted(Counter(y_resampled_smote).items())
Out[29]:
[(0, 4674), (1, 4674), (2, 4674)]

X_resampled_adasyn, y_resampled_adasyn = ADASYN().fit_sample(X, y)

sorted(Counter(y_resampled_adasyn).items())
Out[30]:
[(0, 4674), (1, 4674), (2, 4674)]
```

**SMOTE的变体**

相对于基本的SMOTE算法, 关注的是所有的少数类样本，这些情况可能会导致产生次优的决策函数，因此SMOTE就产生了一些变体：这些方法关注在最优化决策函数边界的一些少数类样本，然后在最近邻类的相反方向生成样本。SMOTE函数中的kind参数控制了选择哪种变体, (i)borderline1, (ii)borderline2, (iii)svm:

```text
from imblearn.over_sampling import SMOTE, ADASYN
X_resampled, y_resampled = SMOTE(kind='borderline1').fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[31]:
[(0, 4674), (1, 4674), (2, 4674)]
```

**数学公式**

SMOTE算法与ADASYN都是基于同样的算法来合成新的少数类样本: 对于少数类样本a, 从它的最近邻中选择一个样本b, 然后在两点的连线上随机生成一个新的少数类样本, 不同的是对于少数类样本的选择。

原始的SMOTE: kind='regular' , 随机选取少数类的样本。

The **borderline** SMOTE: kind='borderline1' or kind='borderline2'

此时, 少数类的样本分为三类: (i) 噪音样本(noise), 该少数类的所有最近邻样本都来自于不同于样本a的其他类别; (ii) 危险样本(in danger), 至少一半的最近邻样本来自于同一类(不同于a的类别); (iii) 安全样本(safe), 所有的最近邻样本都来自于同一个类.

这两种类型的SMOTE使用的是危险样本来生成新的样本数据, 对于 **Borderline-1** SMOTE, 最近邻中的随机样本b与该少数类样本a来自于不同的类; 对于 **Borderline-2** SMOTE , 随机样本b可以是属于任何一个类的样本; **SVM** SMOTE: kind='svm', 使用支持向量机分类器产生支持向量然后再生成新的少数类样本。

**欠采样(Under-sampling)**

什么是欠采样？

当原始数据的分类极不均衡时，如下图

![](https://pic3.zhimg.com/80/v2-6c9c9c8ab7d1265c450adc5c3334011a_720w.jpg)

我们要想用这样的数据去建模显然是存在问题的。尤其是在我们更关心少数类的问题的时候数据分类不均衡会更加的突出，例如，信用卡诈骗、病例分析等。在这样的数据分布的情况下，运用机器学习算法的预测模型可能会无法做出准确的预测，最后的模型显然是趋向于预测多数集的，少数集可能会被当做噪点或被忽视，相比多数集，少数集被错分的可能性很大。从本质上讲，机器学习算法就是从大量的数据集中通过计算得到某些经验，进而判定某些数据的正常与否。但是，不均衡数据集，显然少数类的数量太少，模型会更倾向于多数集。

**常用的下采样方法**

1. 随机下采样

随机欠采样的思想同样比较简单，就是从多数类样本中随机选取一些剔除掉。这种方法的缺点是被剔除的样本可能包含着一些重要信息，致使学习出来的模型效果不好。

2. EasyEnsemble 和 BalanceCascade

EasyEnsemble将多数类样本随机划分成n个子集，每个子集的数量等于少数类样本的数量，这相当于欠采样。接着将每个子集与少数类样本结合起来分别训练一个模型，最后将n个模型集成，这样虽然每个子集的样本少于总体样本，但集成后总信息量并不减少。

如果说EasyEnsemble是基于无监督的方式从多数类样本中生成子集进行欠采样，那么BalanceCascade则是采用了有监督结合Boosting的方式（Boosting方法是一种用来提高弱分类算法准确度的方法,这种方法通过构造一个预测函数系列,然后以一定的方式将他们组合成一个预测函数）。在第n轮训练中，将从多数类样本中抽样得来的子集与少数类样本结合起来训练一个基学习器H，训练完后多数类中能被H正确分类的样本会被剔除。在接下来的第n+1轮中，从被剔除后的多数类样本中产生子集用于与少数类样本结合起来训练，最后将不同的基学习器集成起来。BalanceCascade的有监督表现在每一轮的基学习器起到了在多数类中选择样本的作用，而其Boosting特点则体现在每一轮丢弃被正确分类的样本，进而后续基学习器会更注重那些之前分类错误的样本。

3. 原型生成(prototype generation)

给定数据集S, 原型生成算法将生成一个子集S’, 其中|S’| < |S|, 但是子集并非来自于原始数据集。意思就是说: 原型生成方法将减少数据集的样本数量, 剩下的样本是由原始数据集生成的, 而不是直接来源于原始数据集。

ClusterCentroids函数实现了上述功能: 每一个类别的样本都会用`K-Means`算法的中心点来进行合成, 而不是随机从原始样本进行抽取。

```text
from imblearn.under_sampling import ClusterCentroids

cc = ClusterCentroids(random_state=0)
X_resampled, y_resampled = cc.fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[32]:
[(0, 64), (1, 64), (2, 64)]
```

ClusterCentroids函数提供了一种很高效的方法来减少样本的数量, 但需要注意的是, 该方法要求原始数据集最好能聚类成簇. 此外, 中心点的数量应该设置好, 这样下采样的簇能很好地代表原始数据。

4. 原型选择(prototype selection)

与原型生成不同的是, 原型选择算法是直接从原始数据集中进行抽取. 抽取的方法大概可以分为两类: (i) 可控的下采样技术(the controlled under-sampling techniques) ; (ii) the cleaning under-sampling techniques; 第一类的方法可以由用户指定下采样抽取的子集中样本的数量; 第二类方法则不接受这种用户的干预.

Controlled under-sampling techniques

RandomUnderSampler函数是一种快速并十分简单的方式来平衡各个类别的数据: 随机选取数据的子集.

```text
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=0)
X_resampled, y_resampled = rus.fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[33]:
[(0, 64), (1, 64), (2, 64)]
```

通过设置`RandomUnderSampler`中的`replacement=True`参数, 可以实现自助法(boostrap)抽样.使用默认参数的时候, 采用的是不重复采样;

```text
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUnderSampler(random_state=0, replacement=True)
X_resampled, y_resampled = rus.fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[33]:
[(0, 64), (1, 64), (2, 64)]

np.vstack({tuple(row) for row in X_resampled}).shape
Out[34]:
(181L, 2L)
```

NearMiss函数则添加了一些启发式(heuristic)的规则来选择样本, 通过设定version参数来实现三种启发式的规则。NearMiss本质上是一种原型选择(prototype selection)方法，即从多数类样本中选取最具代表性的样本用于训练，主要是为了缓解随机欠采样中的信息丢失问题。NearMiss采用一些启发式的规则来选择样本，根据规则的不同可分为3类：

NearMiss-1：选择到最近的K个少数类样本平均距离最近的多数类样本

NearMiss-2：选择到最远的K个少数类样本平均距离最近的多数类样本

NearMiss-3：对于每个少数类样本选择K个最近的多数类样本，目的是保证每个少数类样本都被多数类样本包围

NearMiss-1和NearMiss-2的计算开销很大，因为需要计算每个多类别样本的K近邻点。另外，NearMiss-1易受离群点的影响，如下面第二幅图中合理的情况是处于边界附近的多数类样本会被选中，然而由于右下方一些少数类离群点的存在，其附近的多数类样本就被选择了。相比之下NearMiss-2和NearMiss-3不易产生这方面的问题。

![](https://pic3.zhimg.com/80/v2-37aa9363d0c57fa581bad6615f33d252_720w.jpg)

```text
from imblearn.under_sampling import NearMiss
nm1 = NearMiss(random_state=0, version=1)
X_resampled_nm1, y_resampled = nm1.fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[35]:
[(0, 64), (1, 64), (2, 64)]
```

EditedNearestNeighbours这种方法应用最近邻算法来编辑(edit)数据集, 找出那些与邻居不太友好的样本然后移除. 对于每一个要进行下采样的样本, 那些不满足一些准则的样本将会被移除; 他们的绝大多数(kind_sel='mode')或者全部(kind_sel='all')的近邻样本都属于同一个类, 这些样本会被保留在数据集中.

```text
print sorted(Counter(y).items())

from imblearn.under_sampling import EditedNearestNeighbours
enn = EditedNearestNeighbours(random_state=0)
X_resampled, y_resampled = enn.fit_sample(X, y)

print sorted(Counter(y_resampled).items())

Out[36]:
[(0, 64), (1, 262), (2, 4674)]
[(0, 64), (1, 213), (2, 4568)]
```

CondensedNearestNeighbour方法对噪音数据敏感。

NeighbourhoodCleaningRule算法主要关注如何清洗数据而不是筛选(considering)，该算法将使EditedNearestNeighbours和3-NN分类器结果拒绝的样本之间的并集。

InstanceHardnessThreshold是一种很特殊的方法, 是在数据上运用一种分类器, 然后将概率低于阈值的样本剔除掉。

在之前的SMOTE方法中, 当由边界的样本与其他样本进行过采样差值时, 很容易生成一些噪音数据. 因此, 在过采样之后需要对样本进行清洗. 文中涉及到的EditedNearestNeighbours与TomekLink方法就能实现上述的要求. 所以就有了两种结合过采样与下采样的方法: (i)`[SMOTETomek](https://link.zhihu.com/?target=http%3A//contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.combine.SMOTETomek.html%23imblearn.combine.SMOTETomek)`and (ii)`[SMOTEENN](https://link.zhihu.com/?target=http%3A//contrib.scikit-learn.org/imbalanced-learn/stable/generated/imblearn.combine.SMOTEENN.html%23imblearn.combine.SMOTEENN)`。

```text
from imblearn.combine import SMOTEENN
smote_enn = SMOTEENN(random_state=0)
X_resampled, y_resampled = smote_enn.fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[40]:
[(0, 4060), (1, 4381), (2, 3502)]

from imblearn.combine import SMOTETomek
smote_tomek = SMOTETomek(random_state=0)
X_resampled, y_resampled = smote_tomek.fit_sample(X, y)

print sorted(Counter(y_resampled).items())
Out[40]:
[(0, 4499), (1, 4566), (2, 4413)]
```

Ensemble的例子

一个不均衡的数据集能够通过多个均衡的子集来实现均衡, `[imblearn.ensemble](https://link.zhihu.com/?target=http%3A//contrib.scikit-learn.org/imbalanced-learn/stable/api.html%23module-imblearn.ensemble)`模块能实现上述功能。EasyEnsemble通过对原始的数据集进行随机下采样实现对数据集进行集成。

```text
from imblearn.ensemble import EasyEnsemble
ee = EasyEnsemble(random_state=0, n_subsets=10)
X_resampled, y_resampled = ee.fit_sample(X, y)

print X_resampled.shape
print sorted(Counter(y_resampled[0]).items())
Out[40]:
(10L, 192L, 2L)
[(0, 64), (1, 64), (2, 64)]
```

EasyEnsemble有两个很重要的参数: (i) n_subsets控制的是子集的个数 and (ii) replacement 决定是有放回还是无放回的随机采样。

与上述方法不同的是, BalanceCascade(级联平衡)的方法通过使用分类器(estimator参数)来确保那些被错分类的样本在下一次进行子集选取的时候也能被采样到. 同样, n_max_subset参数控制子集的个数, 以及可以通过设置bootstrap=True来使用bootstraping(自助法)。

```text
from imblearn.ensemble import BalanceCascade
from sklearn.linear_model import LogisticRegression
bc = BalanceCascade(random_state=0,
                    estimator=LogisticRegression(random_state=0),
                    n_max_subset=4)
X_resampled, y_resampled = bc.fit_sample(X, y)

print X_resampled.shape

print sorted(Counter(y_resampled[0]).items())
Out[41]:
(4L, 192L, 2L)
[(0, 64), (1, 64), (2, 64)]
```